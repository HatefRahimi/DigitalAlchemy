{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reaction Classification Project - Dataset Discovery\n",
    "\n",
    "**Goal:** Find and explore ORD datasets for Suzuki-Miyaura coupling and C-N coupling reactions to build a reaction classifier.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "from ord_schema import message_helpers\n",
    "from ord_schema.proto import dataset_pb2, reaction_pb2\n",
    "from glob import glob\n",
    "import gzip\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scan ORD Repository for Datasets\n",
    "\n",
    "First, let's find all available datasets and check their names/descriptions for our target reaction types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to your ord-data repository\n",
    "# UPDATE THIS PATH if your repo is in a different location\n",
    "ORD_DATA_PATH = \"ord-data/data\"\n",
    "\n",
    "# Find all dataset files\n",
    "dataset_files = glob(os.path.join(ORD_DATA_PATH, \"**/*.pb.gz\"), recursive=True)\n",
    "print(f\"Found {len(dataset_files)} dataset files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keywords to search for in dataset names/descriptions\n",
    "SUZUKI_KEYWORDS = [\"suzuki\", \"miyaura\", \"suzuki-miyaura\", \"boronic\", \"boronate\"]\n",
    "CN_KEYWORDS = [\"c-n coupling\", \"c-n bond\", \"buchwald\", \"hartwig\", \"amination\", \"ullmann\"]\n",
    "\n",
    "def check_keywords(text, keywords):\n",
    "    \"\"\"Check if any keyword is in the text (case-insensitive).\"\"\"\n",
    "    if text is None:\n",
    "        return False\n",
    "    text_lower = text.lower()\n",
    "    return any(kw in text_lower for kw in keywords)\n",
    "\n",
    "def get_dataset_info(filepath):\n",
    "    \"\"\"Load dataset and extract basic info.\"\"\"\n",
    "    try:\n",
    "        dataset = message_helpers.load_message(filepath, dataset_pb2.Dataset)\n",
    "        return {\n",
    "            \"filepath\": filepath,\n",
    "            \"dataset_id\": dataset.dataset_id,\n",
    "            \"name\": dataset.name,\n",
    "            \"description\": dataset.description,\n",
    "            \"num_reactions\": len(dataset.reactions)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan all datasets and categorize\n",
    "suzuki_datasets = []\n",
    "cn_datasets = []\n",
    "other_datasets = []\n",
    "\n",
    "print(\"Scanning datasets (this may take a few minutes)...\")\n",
    "\n",
    "for filepath in tqdm(dataset_files):\n",
    "    info = get_dataset_info(filepath)\n",
    "    if info is None:\n",
    "        continue\n",
    "    \n",
    "    # Check name and description for keywords\n",
    "    text_to_check = f\"{info['name']} {info['description']}\"\n",
    "    \n",
    "    if check_keywords(text_to_check, SUZUKI_KEYWORDS):\n",
    "        suzuki_datasets.append(info)\n",
    "    elif check_keywords(text_to_check, CN_KEYWORDS):\n",
    "        cn_datasets.append(info)\n",
    "    else:\n",
    "        other_datasets.append(info)\n",
    "\n",
    "print(f\"\\nFound:\")\n",
    "print(f\"  - Suzuki-Miyaura datasets: {len(suzuki_datasets)}\")\n",
    "print(f\"  - C-N coupling datasets: {len(cn_datasets)}\")\n",
    "print(f\"  - Other datasets: {len(other_datasets)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore Found Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Suzuki-Miyaura datasets\n",
    "print(\"=\" * 80)\n",
    "print(\"SUZUKI-MIYAURA DATASETS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "suzuki_df = pd.DataFrame(suzuki_datasets)\n",
    "if len(suzuki_df) > 0:\n",
    "    suzuki_df = suzuki_df.sort_values(\"num_reactions\", ascending=False)\n",
    "    for i, row in suzuki_df.iterrows():\n",
    "        print(f\"\\nðŸ“ {row['name'][:80]}...\" if len(str(row['name'])) > 80 else f\"\\nðŸ“ {row['name']}\")\n",
    "        print(f\"   Reactions: {row['num_reactions']}\")\n",
    "        print(f\"   ID: {row['dataset_id']}\")\n",
    "        desc = str(row['description'])[:150] + \"...\" if len(str(row['description'])) > 150 else row['description']\n",
    "        print(f\"   Description: {desc}\")\n",
    "else:\n",
    "    print(\"No Suzuki-Miyaura datasets found with current keywords.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display C-N coupling datasets\n",
    "print(\"=\" * 80)\n",
    "print(\"C-N COUPLING DATASETS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "cn_df = pd.DataFrame(cn_datasets)\n",
    "if len(cn_df) > 0:\n",
    "    cn_df = cn_df.sort_values(\"num_reactions\", ascending=False)\n",
    "    for i, row in cn_df.iterrows():\n",
    "        print(f\"\\nðŸ“ {row['name'][:80]}...\" if len(str(row['name'])) > 80 else f\"\\nðŸ“ {row['name']}\")\n",
    "        print(f\"   Reactions: {row['num_reactions']}\")\n",
    "        print(f\"   ID: {row['dataset_id']}\")\n",
    "        desc = str(row['description'])[:150] + \"...\" if len(str(row['description'])) > 150 else row['description']\n",
    "        print(f\"   Description: {desc}\")\n",
    "else:\n",
    "    print(\"No C-N coupling datasets found with current keywords.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "total_suzuki = sum(d['num_reactions'] for d in suzuki_datasets)\n",
    "total_cn = sum(d['num_reactions'] for d in cn_datasets)\n",
    "\n",
    "print(f\"\\nSuzuki-Miyaura:\")\n",
    "print(f\"  - {len(suzuki_datasets)} datasets\")\n",
    "print(f\"  - {total_suzuki} total reactions\")\n",
    "\n",
    "print(f\"\\nC-N Coupling:\")\n",
    "print(f\"  - {len(cn_datasets)} datasets\")\n",
    "print(f\"  - {total_cn} total reactions\")\n",
    "\n",
    "print(f\"\\nCombined: {total_suzuki + total_cn} reactions for classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Select Datasets for Training\n",
    "\n",
    "Based on the above exploration, select which datasets to use. Ideally pick datasets that:\n",
    "- Have enough reactions (>100 each)\n",
    "- Have yield data available\n",
    "- Are reasonably balanced between classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After reviewing the output above, manually select the datasets you want to use\n",
    "# Fill in the filepaths from the discovered datasets\n",
    "\n",
    "SELECTED_SUZUKI = [\n",
    "    # Add filepaths here, e.g.:\n",
    "    # \"ord-data/data/xx/ord_dataset-xxxxx.pb.gz\",\n",
    "]\n",
    "\n",
    "SELECTED_CN = [\n",
    "    # Add filepaths here, e.g.:\n",
    "    # \"ord-data/data/xx/ord_dataset-xxxxx.pb.gz\",\n",
    "]\n",
    "\n",
    "print(f\"Selected {len(SELECTED_SUZUKI)} Suzuki datasets\")\n",
    "print(f\"Selected {len(SELECTED_CN)} C-N coupling datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deep Dive: Inspect a Dataset\n",
    "\n",
    "Before committing, let's look at the structure of a specific dataset to understand what features are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_dataset(filepath, n_samples=3):\n",
    "    \"\"\"Inspect the structure and content of a dataset.\"\"\"\n",
    "    dataset = message_helpers.load_message(filepath, dataset_pb2.Dataset)\n",
    "    \n",
    "    print(f\"Dataset: {dataset.name}\")\n",
    "    print(f\"Total reactions: {len(dataset.reactions)}\")\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    \n",
    "    # Check what fields are populated across reactions\n",
    "    has_yield = 0\n",
    "    has_temperature = 0\n",
    "    has_time = 0\n",
    "    input_roles = defaultdict(int)\n",
    "    \n",
    "    for rxn in dataset.reactions:\n",
    "        # Check for yield\n",
    "        for outcome in rxn.outcomes:\n",
    "            for product in outcome.products:\n",
    "                for m in product.measurements:\n",
    "                    if m.type == reaction_pb2.ProductMeasurement.YIELD:\n",
    "                        has_yield += 1\n",
    "                        break\n",
    "        \n",
    "        # Check conditions\n",
    "        if rxn.conditions.HasField(\"temperature\"):\n",
    "            has_temperature += 1\n",
    "        \n",
    "        # Check for reaction time in outcomes\n",
    "        for outcome in rxn.outcomes:\n",
    "            if outcome.HasField(\"reaction_time\"):\n",
    "                has_time += 1\n",
    "                break\n",
    "        \n",
    "        # Count input roles\n",
    "        for key, inp in rxn.inputs.items():\n",
    "            for comp in inp.components:\n",
    "                role = reaction_pb2.ReactionRole.ReactionRoleType.Name(comp.reaction_role)\n",
    "                input_roles[role] += 1\n",
    "    \n",
    "    n = len(dataset.reactions)\n",
    "    print(f\"\\nData availability:\")\n",
    "    print(f\"  - Yield data: {has_yield}/{n} ({100*has_yield/n:.1f}%)\")\n",
    "    print(f\"  - Temperature: {has_temperature}/{n} ({100*has_temperature/n:.1f}%)\")\n",
    "    print(f\"  - Reaction time: {has_time}/{n} ({100*has_time/n:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nInput roles found:\")\n",
    "    for role, count in sorted(input_roles.items(), key=lambda x: -x[1]):\n",
    "        print(f\"  - {role}: {count}\")\n",
    "    \n",
    "    # Show sample reactions\n",
    "    print(f\"\\n\" + \"=\" * 40)\n",
    "    print(f\"Sample reactions:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    for i, rxn in enumerate(dataset.reactions[:n_samples]):\n",
    "        print(f\"\\n--- Reaction {i+1} ---\")\n",
    "        for key, inp in rxn.inputs.items():\n",
    "            print(f\"  Input '{key}':\")\n",
    "            for comp in inp.components:\n",
    "                role = reaction_pb2.ReactionRole.ReactionRoleType.Name(comp.reaction_role)\n",
    "                # Get identifier\n",
    "                ident_str = \"[no identifier]\"\n",
    "                for ident in comp.identifiers:\n",
    "                    ident_type = reaction_pb2.CompoundIdentifier.CompoundIdentifierType.Name(ident.type)\n",
    "                    if ident_type in [\"SMILES\", \"NAME\"]:\n",
    "                        ident_str = f\"{ident_type}: {ident.value[:50]}{'...' if len(ident.value) > 50 else ''}\"\n",
    "                        break\n",
    "                print(f\"    - {role}: {ident_str}\")\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect one of the found Suzuki datasets (if any)\n",
    "if len(suzuki_datasets) > 0:\n",
    "    # Pick the largest one\n",
    "    largest_suzuki = max(suzuki_datasets, key=lambda x: x['num_reactions'])\n",
    "    print(\"Inspecting largest Suzuki-Miyaura dataset:\\n\")\n",
    "    _ = inspect_dataset(largest_suzuki['filepath'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect one of the found C-N coupling datasets (if any)\n",
    "if len(cn_datasets) > 0:\n",
    "    # Pick the largest one\n",
    "    largest_cn = max(cn_datasets, key=lambda x: x['num_reactions'])\n",
    "    print(\"Inspecting largest C-N coupling dataset:\\n\")\n",
    "    _ = inspect_dataset(largest_cn['filepath'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Next Steps\n",
    "\n",
    "After running this notebook, you will know:\n",
    "1. How many datasets exist for each reaction type\n",
    "2. How many reactions are available\n",
    "3. What features are consistently available (yield, temperature, roles, etc.)\n",
    "\n",
    "The next notebook will:\n",
    "1. Load the selected datasets\n",
    "2. Extract features (SMILES â†’ fingerprints or one-hot encoding)\n",
    "3. Build and train the classification model\n",
    "\n",
    "---\n",
    "\n",
    "**Note:** If the keyword search doesn't find enough datasets, we may need to:\n",
    "- Expand the keywords\n",
    "- Look at the reaction SMILES patterns directly\n",
    "- Check the `reaction_type` field in the reaction messages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

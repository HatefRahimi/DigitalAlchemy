{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yield Prediction: Ahneman Buchwald-Hartwig Dataset\n",
    "\n",
    "**Goal:** Predict reaction yield using ML\n",
    "\n",
    "**Approach:** Following ORD creator's method:\n",
    "1. Convert reactions to DataFrame using `messages_to_dataframe()`\n",
    "2. One-hot encode categorical inputs (aryl halide, amine, ligand, base, additive)\n",
    "3. Train neural network to predict yield\n",
    "\n",
    "**Why this IS real ML:** Same substrates + different conditions → different yields\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from ord_schema import message_helpers, validations\n",
    "from ord_schema.proto import dataset_pb2, reaction_pb2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Ahneman dataset\n",
    "dataset_path = \"ord-data/data/46/ord_dataset-46ff9a32d9e04016b9380b1b1ef949c3.pb.gz\"\n",
    "data = message_helpers.load_message(dataset_path, dataset_pb2.Dataset)\n",
    "\n",
    "print(f\"Dataset: {data.name}\")\n",
    "print(f\"Description: {data.description}\")\n",
    "print(f\"Reactions: {len(data.reactions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame using ORD helper\n",
    "df = message_helpers.messages_to_dataframe(data.reactions, drop_constant_columns=True)\n",
    "\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(f\"\\nColumns ({len(df.columns)}):\")\n",
    "for col in df.columns:\n",
    "    print(f\"  {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Select Modeling Columns\n",
    "\n",
    "We need to identify:\n",
    "- Input columns (SMILES for reactants, catalysts, etc.)\n",
    "- Output column (yield)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns containing SMILES identifiers and yield\n",
    "print(\"Looking for input and output columns...\\n\")\n",
    "\n",
    "smiles_cols = [col for col in df.columns if 'identifiers' in col and 'SMILES' in str(df[col].iloc[0])]\n",
    "yield_cols = [col for col in df.columns if 'percentage' in col.lower() or 'yield' in col.lower()]\n",
    "\n",
    "print(\"Potential SMILES columns:\")\n",
    "for col in smiles_cols[:10]:\n",
    "    print(f\"  {col}\")\n",
    "\n",
    "print(f\"\\nPotential yield columns:\")\n",
    "for col in yield_cols:\n",
    "    print(f\"  {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore the input structure more carefully\n",
    "# Find columns with 'inputs' to understand the reaction components\n",
    "\n",
    "input_cols = [col for col in df.columns if col.startswith('inputs')]\n",
    "print(f\"Input columns ({len(input_cols)}):\")\n",
    "for col in sorted(set([c.split('.')[0] + '.' + c.split('.')[1] if '.' in c else c.split('[')[0] + '[' + c.split('[')[1].split(']')[0] + ']' for c in input_cols])):\n",
    "    print(f\"  {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore one reaction to understand structure\n",
    "rxn = data.reactions[0]\n",
    "\n",
    "print(\"=== REACTION INPUTS ===\")\n",
    "for key, inp in rxn.inputs.items():\n",
    "    print(f\"\\n{key}:\")\n",
    "    for comp in inp.components:\n",
    "        role = reaction_pb2.ReactionRole.ReactionRoleType.Name(comp.reaction_role)\n",
    "        for ident in comp.identifiers:\n",
    "            ident_type = reaction_pb2.CompoundIdentifier.CompoundIdentifierType.Name(ident.type)\n",
    "            if ident_type == \"SMILES\":\n",
    "                print(f\"  {role}: {ident.value[:60]}\")\n",
    "\n",
    "print(\"\\n=== YIELD ===\")\n",
    "for outcome in rxn.outcomes:\n",
    "    for product in outcome.products:\n",
    "        for meas in product.measurements:\n",
    "            if meas.type == reaction_pb2.ProductMeasurement.ProductMeasurementType.YIELD:\n",
    "                print(f\"  Yield: {meas.percentage.value}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data manually for cleaner control\n",
    "def extract_reaction_data(reaction):\n",
    "    \"\"\"Extract key components and yield from a reaction.\"\"\"\n",
    "    data = {}\n",
    "    \n",
    "    # Extract inputs by key name\n",
    "    for key, inp in reaction.inputs.items():\n",
    "        for comp in inp.components:\n",
    "            for ident in comp.identifiers:\n",
    "                if reaction_pb2.CompoundIdentifier.CompoundIdentifierType.Name(ident.type) == \"SMILES\":\n",
    "                    # Use the input key as the column name\n",
    "                    clean_key = key.replace(' ', '_').lower()\n",
    "                    data[clean_key] = ident.value\n",
    "                    break\n",
    "    \n",
    "    # Extract yield\n",
    "    data['yield'] = None\n",
    "    for outcome in reaction.outcomes:\n",
    "        for product in outcome.products:\n",
    "            for meas in product.measurements:\n",
    "                if meas.type == reaction_pb2.ProductMeasurement.ProductMeasurementType.YIELD:\n",
    "                    data['yield'] = meas.percentage.value\n",
    "                    break\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Extract all reactions\n",
    "reaction_data = [extract_reaction_data(rxn) for rxn in tqdm(data.reactions)]\n",
    "df_clean = pd.DataFrame(reaction_data)\n",
    "\n",
    "print(f\"Extracted {len(df_clean)} reactions\")\n",
    "print(f\"Columns: {list(df_clean.columns)}\")\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "print(df_clean.isnull().sum())\n",
    "\n",
    "# Drop rows without yield\n",
    "df_clean = df_clean.dropna(subset=['yield'])\n",
    "print(f\"\\nReactions with yield: {len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yield distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.hist(df_clean['yield'], bins=50, color='#3498db', edgecolor='black', alpha=0.7)\n",
    "ax.set_xlabel('Yield (%)', fontsize=12)\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.set_title('Yield Distribution - Ahneman Dataset', fontsize=14, fontweight='bold')\n",
    "ax.axvline(x=df_clean['yield'].mean(), color='red', linestyle='--', label=f\"Mean: {df_clean['yield'].mean():.1f}%\")\n",
    "ax.axvline(x=df_clean['yield'].median(), color='green', linestyle='--', label=f\"Median: {df_clean['yield'].median():.1f}%\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Yield statistics:\")\n",
    "print(f\"  Min:    {df_clean['yield'].min():.1f}%\")\n",
    "print(f\"  Max:    {df_clean['yield'].max():.1f}%\")\n",
    "print(f\"  Mean:   {df_clean['yield'].mean():.1f}%\")\n",
    "print(f\"  Median: {df_clean['yield'].median():.1f}%\")\n",
    "print(f\"  Std:    {df_clean['yield'].std():.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering: One-Hot Encoding\n",
    "\n",
    "Convert each unique SMILES to a binary feature (one-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify input columns (everything except yield)\n",
    "input_cols = [col for col in df_clean.columns if col != 'yield']\n",
    "\n",
    "print(\"Input columns for modeling:\")\n",
    "for col in input_cols:\n",
    "    n_unique = df_clean[col].nunique()\n",
    "    print(f\"  {col}: {n_unique} unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one-hot encoding\n",
    "# Use short prefixes for cleaner feature names\n",
    "prefix_map = {}\n",
    "for i, col in enumerate(input_cols):\n",
    "    # Create short prefix\n",
    "    prefix_map[col] = col[:10]\n",
    "\n",
    "ohe_df = pd.get_dummies(df_clean[input_cols], prefix=[prefix_map[c] for c in input_cols])\n",
    "\n",
    "# Add normalized yield (0-1)\n",
    "ohe_df['yield'] = df_clean['yield'].values / 100\n",
    "\n",
    "print(f\"One-hot encoded shape: {ohe_df.shape}\")\n",
    "print(f\"Features: {ohe_df.shape[1] - 1}\")\n",
    "ohe_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Train/Val/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create numpy arrays\n",
    "X = ohe_df.drop(columns=['yield']).values\n",
    "y = ohe_df['yield'].values\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "\n",
    "# Split: 60% train, 10% validation, 30% test\n",
    "_X_train, X_test, _y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(_X_train, _y_train, test_size=0.1/0.7, random_state=42)\n",
    "\n",
    "print(f\"\\nTrain: {X_train.shape[0]}\")\n",
    "print(f\"Val:   {X_val.shape[0]}\")\n",
    "print(f\"Test:  {X_test.shape[0]}\")\n",
    "print(f\"Total: {X_train.shape[0] + X_val.shape[0] + X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, try sklearn models as baseline\n",
    "print(\"Training sklearn models...\\n\")\n",
    "\n",
    "sklearn_models = {\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42),\n",
    "}\n",
    "\n",
    "sklearn_results = {}\n",
    "\n",
    "for name, model in sklearn_models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Metrics (convert back to percentage)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test * 100, y_pred * 100))\n",
    "    mae = mean_absolute_error(y_test * 100, y_pred * 100)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    sklearn_results[name] = {'model': model, 'rmse': rmse, 'mae': mae, 'r2': r2, 'y_pred': y_pred}\n",
    "    print(f\"  RMSE: {rmse:.2f}%, MAE: {mae:.2f}%, R²: {r2:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network (following ORD creator's approach)\n",
    "print(\"Training Neural Network...\\n\")\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "batch_size = 100\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train.astype(np.float32), y_train.astype(np.float32))).batch(batch_size)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val.astype(np.float32), y_val.astype(np.float32))).batch(batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test.astype(np.float32), y_test.astype(np.float32))).batch(batch_size)\n",
    "\n",
    "# Build model (same architecture as ORD creators)\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Input(shape=(X_train.shape[1],)),\n",
    "    keras.layers.Dense(50, activation='sigmoid'),\n",
    "    keras.layers.Dense(7, activation='sigmoid'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(0.005),\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    metrics=[keras.metrics.RootMeanSquaredError()]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with early stopping based on validation loss\n",
    "epochs = 300\n",
    "\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath='best_model.keras',\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=30,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[checkpoint_callback, early_stopping],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(history.history['loss'], label='Training Loss', color='blue')\n",
    "ax.plot(history.history['val_loss'], label='Validation Loss', color='green')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss (MSE)')\n",
    "ax.set_title('Training History', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, max(history.history['loss'][5:]) * 1.5)  # Zoom in after initial epochs\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_epoch = np.argmin(history.history['val_loss'])\n",
    "print(f\"Best epoch: {best_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Neural Network on test set\n",
    "y_pred_nn = model.predict(test_dataset).flatten()\n",
    "\n",
    "# Metrics\n",
    "rmse_nn = np.sqrt(mean_squared_error(y_test * 100, y_pred_nn * 100))\n",
    "mae_nn = mean_absolute_error(y_test * 100, y_pred_nn * 100)\n",
    "r2_nn = r2_score(y_test, y_pred_nn)\n",
    "\n",
    "print(f\"Neural Network Test Results:\")\n",
    "print(f\"  RMSE: {rmse_nn:.2f}%\")\n",
    "print(f\"  MAE:  {mae_nn:.2f}%\")\n",
    "print(f\"  R²:   {r2_nn:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_results = {\n",
    "    **sklearn_results,\n",
    "    'Neural Network': {'rmse': rmse_nn, 'mae': mae_nn, 'r2': r2_nn, 'y_pred': y_pred_nn}\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(all_results.keys()),\n",
    "    'RMSE (%)': [all_results[m]['rmse'] for m in all_results],\n",
    "    'MAE (%)': [all_results[m]['mae'] for m in all_results],\n",
    "    'R²': [all_results[m]['r2'] for m in all_results]\n",
    "}).sort_values('RMSE (%)')\n",
    "\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Baseline comparison\n",
    "mean_baseline_rmse = np.sqrt(mean_squared_error(y_test * 100, np.full_like(y_test, y_train.mean()) * 100))\n",
    "print(f\"\\nBaseline (predict mean): RMSE = {mean_baseline_rmse:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize best model predictions\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "y_pred_best = all_results[best_model_name]['y_pred']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Actual vs Predicted\n",
    "ax = axes[0]\n",
    "ax.scatter(y_test * 100, y_pred_best * 100, alpha=0.5, s=10, c='#3498db')\n",
    "ax.plot([0, 100], [0, 100], 'r--', label='Perfect prediction')\n",
    "ax.set_xlabel('Actual Yield (%)', fontsize=12)\n",
    "ax.set_ylabel('Predicted Yield (%)', fontsize=12)\n",
    "ax.set_title(f'{best_model_name}: Actual vs Predicted', fontsize=14, fontweight='bold')\n",
    "ax.set_xlim(-5, 105)\n",
    "ax.set_ylim(-5, 105)\n",
    "ax.legend()\n",
    "\n",
    "# Residual distribution\n",
    "ax = axes[1]\n",
    "residuals = (y_test - y_pred_best) * 100\n",
    "ax.hist(residuals, bins=50, color='#3498db', edgecolor='black', alpha=0.7)\n",
    "ax.set_xlabel('Residual (Actual - Predicted) %', fontsize=12)\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.set_title('Residual Distribution', fontsize=14, fontweight='bold')\n",
    "ax.axvline(x=0, color='red', linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Prediction accuracy\n",
    "errors = np.abs(y_test - y_pred_best) * 100\n",
    "print(f\"\\nPrediction accuracy ({best_model_name}):\")\n",
    "print(f\"  Within ±5%:  {100*np.mean(errors <= 5):.1f}% of predictions\")\n",
    "print(f\"  Within ±10%: {100*np.mean(errors <= 10):.1f}% of predictions\")\n",
    "print(f\"  Within ±20%: {100*np.mean(errors <= 20):.1f}% of predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from Random Forest\n",
    "rf = sklearn_results['Random Forest']['model']\n",
    "feature_names = ohe_df.drop(columns=['yield']).columns\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# Get top 20\n",
    "indices = np.argsort(importances)[::-1][:20]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.bar(range(20), importances[indices], color='#3498db', edgecolor='black')\n",
    "ax.set_xticks(range(20))\n",
    "ax.set_xticklabels([feature_names[i][:20] for i in indices], rotation=45, ha='right')\n",
    "ax.set_ylabel('Importance')\n",
    "ax.set_title('Top 20 Most Important Features for Yield Prediction', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "### What we learned:\n",
    "1. **This IS real ML** - predicting yield from structure/conditions has no simple rules\n",
    "2. **One-hot encoding works** - treats each unique molecule as a categorical feature\n",
    "3. **Model comparison** - see which approach works best for this dataset\n",
    "\n",
    "### Limitations:\n",
    "- One-hot encoding doesn't generalize to new molecules\n",
    "- For new substrates, would need molecular fingerprints or other representations\n",
    "\n",
    "### Next steps:\n",
    "- Test on external dataset to check generalization\n",
    "- Try molecular fingerprints instead of one-hot encoding\n",
    "- Add more condition features (temperature, time, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
